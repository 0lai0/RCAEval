#!/usr/bin/env python3
"""
GRUMVGCä½¿ç”¨ç¤ºä¾‹
å±•ç¤ºå¦‚ä½•åœ¨RCAEvalæ¡†æ¶ä¸­ä½¿ç”¨GRUMVGCé€²è¡Œå¤šæ¨¡æ…‹æ ¹å› åˆ†æ
"""

import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pathlib import Path
import sys

# ç¢ºä¿èƒ½å¤ å°å…¥RCAEval
sys.path.insert(0, str(Path(__file__).parent))

def create_synthetic_multimodal_data():
    """å‰µå»ºåˆæˆçš„å¤šæ¨¡æ…‹å¾®æœå‹™ç›£æ§æ•¸æ“š"""
    
    print("ğŸ”§ Creating synthetic multimodal data...")
    
    np.random.seed(42)
    n_samples = 300
    time_points = np.arange(n_samples)
    
    # å®šç¾©æœå‹™åˆ—è¡¨
    services = ['frontend', 'auth', 'payment', 'inventory', 'notification']
    
    # å‰µå»ºMetricsæ•¸æ“š
    metric_data = {'time': time_points}
    
    for service in services:
        # CPUä½¿ç”¨ç‡ (0-1)
        base_cpu = np.random.uniform(0.2, 0.4)
        metric_data[f'{service}_cpu'] = base_cpu + 0.1 * np.sin(time_points * 0.1) + np.random.normal(0, 0.05, n_samples)
        
        # å…§å­˜ä½¿ç”¨ç‡ (0-1)  
        base_memory = np.random.uniform(0.4, 0.6)
        metric_data[f'{service}_memory'] = base_memory + 0.05 * np.cos(time_points * 0.08) + np.random.normal(0, 0.03, n_samples)
        
        # ç¶²çµ¡æµé‡ (MB/s)
        base_network = np.random.uniform(10, 50)
        metric_data[f'{service}_network'] = base_network + 10 * np.sin(time_points * 0.05) + np.random.normal(0, 2, n_samples)
    
    # å‰µå»ºLogsæ™‚é–“åºåˆ—æ•¸æ“š
    logts_data = {'time': time_points}
    
    for service in services:
        # éŒ¯èª¤æ—¥èªŒè¨ˆæ•¸
        base_errors = np.random.poisson(1, n_samples)
        logts_data[f'{service}_error_count'] = base_errors
        
        # è«‹æ±‚æ—¥èªŒè¨ˆæ•¸
        base_requests = np.random.poisson(20, n_samples)
        logts_data[f'{service}_request_count'] = base_requests
        
        # è­¦å‘Šæ—¥èªŒè¨ˆæ•¸
        base_warnings = np.random.poisson(3, n_samples)
        logts_data[f'{service}_warning_count'] = base_warnings
    
    # æ¨¡æ“¬ç•°å¸¸å ´æ™¯ï¼špaymentæœå‹™åœ¨t=200å¾Œå‡ºç¾å•é¡Œ
    inject_time = 200
    
    print(f"ğŸ’¥ Injecting anomaly at time {inject_time}")
    
    # Paymentæœå‹™CPUå’Œå…§å­˜ç•°å¸¸å¢åŠ 
    metric_data['payment_cpu'][inject_time:] += 0.4
    metric_data['payment_memory'][inject_time:] += 0.3
    
    # Paymentæœå‹™éŒ¯èª¤æ—¥èªŒæ¿€å¢
    logts_data['payment_error_count'][inject_time:] += np.random.poisson(10, n_samples - inject_time)
    
    # ä¸Šæ¸¸æœå‹™(frontend, auth)ä¹Ÿå—åˆ°å½±éŸ¿
    metric_data['frontend_cpu'][inject_time+5:] += 0.2  # å»¶é²5ç§’å½±éŸ¿
    metric_data['auth_cpu'][inject_time+3:] += 0.15     # å»¶é²3ç§’å½±éŸ¿
    
    logts_data['frontend_error_count'][inject_time+5:] += np.random.poisson(3, n_samples - inject_time - 5)
    logts_data['auth_error_count'][inject_time+3:] += np.random.poisson(2, n_samples - inject_time - 3)
    
    # ä¸‹æ¸¸æœå‹™(inventory, notification)ä¹Ÿå—åˆ°å½±éŸ¿
    metric_data['inventory_cpu'][inject_time+8:] += 0.1  # å»¶é²8ç§’å½±éŸ¿
    logts_data['notification_error_count'][inject_time+10:] += np.random.poisson(1, n_samples - inject_time - 10)
    
    # è½‰æ›ç‚ºDataFrame
    metric_df = pd.DataFrame(metric_data)
    logts_df = pd.DataFrame(logts_data)
    
    # ç¢ºä¿æ•¸å€¼åœ¨åˆç†ç¯„åœå…§
    for col in metric_df.columns:
        if col != 'time':
            if 'cpu' in col or 'memory' in col:
                metric_df[col] = np.clip(metric_df[col], 0, 1)
            elif 'network' in col:
                metric_df[col] = np.clip(metric_df[col], 0, None)
    
    for col in logts_df.columns:
        if col != 'time':
            logts_df[col] = np.clip(logts_df[col], 0, None)
    
    print(f"âœ… Created metric data: {metric_df.shape}")
    print(f"âœ… Created logts data: {logts_df.shape}")
    
    return {
        'metric': metric_df,
        'logts': logts_df
    }, inject_time


def visualize_data(multimodal_data, inject_time):
    """å¯è¦–åŒ–å¤šæ¨¡æ…‹æ•¸æ“š"""
    
    print("ğŸ“Š Visualizing multimodal data...")
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('Multimodal Microservice Monitoring Data', fontsize=16)
    
    metric_df = multimodal_data['metric']
    logts_df = multimodal_data['logts']
    
    # ç¹ªè£½CPUä½¿ç”¨ç‡
    ax1 = axes[0, 0]
    cpu_cols = [col for col in metric_df.columns if 'cpu' in col]
    for col in cpu_cols[:3]:  # åªé¡¯ç¤ºå‰3å€‹æœå‹™ä»¥é¿å…åœ–è¡¨éæ–¼æ“æ“ 
        ax1.plot(metric_df['time'], metric_df[col], label=col, alpha=0.7)
    ax1.axvline(x=inject_time, color='red', linestyle='--', alpha=0.8, label='Anomaly Injection')
    ax1.set_title('CPU Usage')
    ax1.set_ylabel('CPU Usage')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # ç¹ªè£½å…§å­˜ä½¿ç”¨ç‡
    ax2 = axes[0, 1]
    memory_cols = [col for col in metric_df.columns if 'memory' in col]
    for col in memory_cols[:3]:
        ax2.plot(metric_df['time'], metric_df[col], label=col, alpha=0.7)
    ax2.axvline(x=inject_time, color='red', linestyle='--', alpha=0.8, label='Anomaly Injection')
    ax2.set_title('Memory Usage')
    ax2.set_ylabel('Memory Usage')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # ç¹ªè£½éŒ¯èª¤æ—¥èªŒè¨ˆæ•¸
    ax3 = axes[1, 0]
    error_cols = [col for col in logts_df.columns if 'error_count' in col]
    for col in error_cols[:3]:
        ax3.plot(logts_df['time'], logts_df[col], label=col, alpha=0.7)
    ax3.axvline(x=inject_time, color='red', linestyle='--', alpha=0.8, label='Anomaly Injection')
    ax3.set_title('Error Log Counts')
    ax3.set_ylabel('Error Count')
    ax3.set_xlabel('Time')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # ç¹ªè£½è«‹æ±‚æ—¥èªŒè¨ˆæ•¸
    ax4 = axes[1, 1]
    request_cols = [col for col in logts_df.columns if 'request_count' in col]
    for col in request_cols[:3]:
        ax4.plot(logts_df['time'], logts_df[col], label=col, alpha=0.7)
    ax4.axvline(x=inject_time, color='red', linestyle='--', alpha=0.8, label='Anomaly Injection')
    ax4.set_title('Request Log Counts')
    ax4.set_ylabel('Request Count')
    ax4.set_xlabel('Time')
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('multimodal_data_visualization.png', dpi=300, bbox_inches='tight')
    print("âœ… Visualization saved as 'multimodal_data_visualization.png'")
    
    return fig


def run_grumvgc_analysis(multimodal_data, inject_time):
    """é‹è¡ŒGRUMVGCæ ¹å› åˆ†æ"""
    
    print("\nğŸš€ Running GRUMVGC analysis...")
    
    try:
        from RCAEval.e2e.grumvgc import grumvgc
        
        # é‹è¡ŒGRUMVGCï¼Œä½¿ç”¨é©ä¸­çš„åƒæ•¸ä»¥å¹³è¡¡æ€§èƒ½å’Œæ•ˆæœ
        result = grumvgc(
            data=multimodal_data,
            inject_time=inject_time,
            dataset="synthetic_microservice",
            embedding_dim=64,       # é©ä¸­çš„åµŒå…¥ç¶­åº¦
            seq_len=40,            # é©ä¸­çš„åºåˆ—é•·åº¦
            num_epochs=15,         # é©ä¸­çš„è¨“ç·´è¼ªæ•¸
            batch_size=16,         # é©ä¸­çš„æ‰¹æ¬¡å¤§å°
            dk_select_useful=True  # å•Ÿç”¨ç‰¹å¾µé¸æ“‡
        )
        
        print("\nğŸ¯ GRUMVGC Analysis Results:")
        print("=" * 50)
        
        # é¡¯ç¤ºåŸºæœ¬ä¿¡æ¯
        print(f"ğŸ“Š Total nodes analyzed: {len(result['node_names'])}")
        print(f"ğŸ”— Causal edges found: {len(result.get('causal_edges', []))}")
        print(f"ğŸ“ Adjacency matrix shape: {result['adj'].shape}")
        
        # é¡¯ç¤ºå‰10å€‹æ ¹å› 
        print(f"\nğŸ† Top 10 Root Causes:")
        print("-" * 30)
        for i, cause in enumerate(result['ranks'][:10], 1):
            # ç°¡åŒ–ç¯€é»åç¨±é¡¯ç¤º
            display_name = cause.replace('metric_', '').replace('logts_', '')
            print(f"{i:2d}. {display_name}")
        
        # é¡¯ç¤ºå› æœé‚Šä¿¡æ¯
        if result.get('causal_edges'):
            print(f"\nğŸ”— Sample Causal Relationships:")
            print("-" * 40)
            for i, (source, target, weight) in enumerate(result['causal_edges'][:5], 1):
                source_clean = source.replace('metric_', '').replace('logts_', '')
                target_clean = target.replace('metric_', '').replace('logts_', '')
                print(f"{i}. {source_clean} â†’ {target_clean} (strength: {weight:.3f})")
        
        # é¡¯ç¤ºåµŒå…¥ä¿¡æ¯
        if 'embeddings_info' in result:
            info = result['embeddings_info']
            print(f"\nğŸ§  Embedding Information:")
            print(f"   Features processed: {info['num_features']}")
            print(f"   Embedding dimension: {info['embedding_dim']}")
        
        # åˆ†æçµæœè³ªé‡
        print(f"\nğŸ“ˆ Analysis Quality Assessment:")
        print("-" * 35)
        
        # æª¢æŸ¥æ˜¯å¦æ­£ç¢ºè­˜åˆ¥äº†paymentæœå‹™ä½œç‚ºæ ¹å› 
        top_5_causes = result['ranks'][:5]
        payment_related = [cause for cause in top_5_causes if 'payment' in cause.lower()]
        
        if payment_related:
            print(f"âœ… Payment service correctly identified in top 5: {payment_related}")
        else:
            print("âš ï¸  Payment service not in top 5 root causes")
        
        # æª¢æŸ¥æ˜¯å¦è­˜åˆ¥äº†ç›¸é—œçš„ä¸Šæ¸¸æœå‹™
        upstream_services = ['frontend', 'auth']
        top_10_causes = result['ranks'][:10]
        upstream_in_top10 = [cause for cause in top_10_causes 
                           if any(service in cause.lower() for service in upstream_services)]
        
        if upstream_in_top10:
            print(f"âœ… Upstream services identified: {upstream_in_top10}")
        else:
            print("âš ï¸  Upstream services not prominently ranked")
        
        return result
        
    except Exception as e:
        print(f"âŒ GRUMVGC analysis failed: {e}")
        import traceback
        traceback.print_exc()
        return None


def compare_with_baseline(multimodal_data, inject_time):
    """èˆ‡åŸºç·šæ–¹æ³•æ¯”è¼ƒ"""
    
    print("\nğŸ“Š Comparing with baseline methods...")
    
    try:
        from RCAEval.e2e.granger_pagerank import granger_pagerank
        
        # ä½¿ç”¨ç°¡å–®çš„grangeræ–¹æ³•ä½œç‚ºåŸºç·š
        # éœ€è¦å°‡å¤šæ¨¡æ…‹æ•¸æ“šåˆä½µç‚ºå–®ä¸€DataFrame
        metric_df = multimodal_data['metric']
        logts_df = multimodal_data['logts']
        
        # åˆä½µæ•¸æ“šï¼ˆç°¡åŒ–è™•ç†ï¼‰
        combined_df = metric_df.copy()
        for col in logts_df.columns:
            if col != 'time':
                combined_df[f'log_{col}'] = logts_df[col]
        
        baseline_result = granger_pagerank(
            data=combined_df,
            inject_time=inject_time,
            dataset="synthetic_microservice"
        )
        
        print("ğŸ”„ Baseline (Granger PageRank) Results:")
        print(f"   Top 5 causes: {baseline_result['ranks'][:5]}")
        
        return baseline_result
        
    except Exception as e:
        print(f"âš ï¸  Baseline comparison failed: {e}")
        return None


def main():
    """ä¸»å‡½æ•¸"""
    
    print("ğŸ¬ GRUMVGC Multimodal Root Cause Analysis Demo")
    print("=" * 55)
    
    # æ­¥é©Ÿ1: å‰µå»ºåˆæˆæ•¸æ“š
    multimodal_data, inject_time = create_synthetic_multimodal_data()
    
    # æ­¥é©Ÿ2: å¯è¦–åŒ–æ•¸æ“š
    try:
        fig = visualize_data(multimodal_data, inject_time)
        plt.show()
    except Exception as e:
        print(f"âš ï¸  Visualization failed: {e}")
    
    # æ­¥é©Ÿ3: é‹è¡ŒGRUMVGCåˆ†æ
    grumvgc_result = run_grumvgc_analysis(multimodal_data, inject_time)
    
    # æ­¥é©Ÿ4: èˆ‡åŸºç·šæ–¹æ³•æ¯”è¼ƒ
    baseline_result = compare_with_baseline(multimodal_data, inject_time)
    
    # æ­¥é©Ÿ5: ç¸½çµ
    print("\nğŸ¯ Analysis Summary:")
    print("=" * 25)
    
    if grumvgc_result:
        print("âœ… GRUMVGC analysis completed successfully")
        print(f"   Identified {len(grumvgc_result['ranks'])} potential root causes")
        print(f"   Found {len(grumvgc_result.get('causal_edges', []))} causal relationships")
    else:
        print("âŒ GRUMVGC analysis failed")
    
    if baseline_result:
        print("âœ… Baseline comparison completed")
    else:
        print("âš ï¸  Baseline comparison unavailable")
    
    print("\nğŸ Demo completed!")
    print("\nNext steps:")
    print("1. Analyze the visualization to understand the anomaly pattern")
    print("2. Review the root cause rankings")
    print("3. Examine the causal relationships discovered")
    print("4. Compare with domain knowledge and ground truth")
    
    return grumvgc_result, baseline_result


if __name__ == "__main__":
    results = main() 